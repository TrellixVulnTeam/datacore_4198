goto comment
b'from re import X\nimport pandas as pd\nimport json\nimport time\nimport sys, os\nimport argostranslate.package, argostranslate.translate\nfrom arango import ArangoClient\n\narango_host = \'http://127.0.0.1:8529/\'\narango_database = \'db_\' + \'db1\'\narango_username = \'root\'\narango_password = \'123456789\'\n\nconfig = {\n    "file_name": "USDJPY_Candlestick_15_M_BID_01.01.2018-31.12.2018.csv",\n    "has_header": True,\n    "import_all_files": True,\n    "used_fields": [\n        0\n    ],\n    "collections": [\n        {\n            "index": 0,\n            "name": "col_person",\n            "name_ar": "\\u0627\\u0641\\u0631\\u0627\\u062f",\n            "fields_indecies": [\n                0\n            ],\n            "fields_names": [\n                "f_name"\n            ],\n            "fields": [\n                {\n                    "name": "f_name",\n                    "name_ar": "\\u0627\\u0644\\u0627\\u0633\\u0645",\n                    "type": "String",\n                    "format": "",\n                    "match": False,\n                    "ff_index": 0\n                }\n            ],\n            "identity_fields": []\n        }\n    ],\n    "edges": []\n}\n\nstart_time = time.time()\nheader_conf = \'infer\'\nif not config[\'has_header\']:\n\theader = None\n\ndf = pd.read_csv(config[\'file_name\'], engine="pyarrow", header=header_conf)\ncol_list = [\'column_\' + str(x) for x in range(1,df.shape[1]+1)]\ndf = df.set_axis(col_list, axis=\'columns\')\n\nprint(\'\\n\' + \'All data:\\n---------------------------\\n\')\nprint(df.head())\n\n\nsession_key = str(round((time.time()-1656924275) * 10000))\ndoc_key = 0\n\ndef generate_key():\n\tglobal doc_key\n\tdoc_key += 1\n\treturn f\'{session_key}.{doc_key}\'\n\t\ndef cast_fields(source):\n\tboolean_map = {\'1\':True,\'true\':True,\'True\':True,\'TRUE\':True,\'yes\':True,\'Yes\':True,\'YES\':True,\'ok\':True,\'Ok\':True,\'OK\':True,\'\xd9\x86\xd8\xb9\xd9\x85\':True,\'\xd8\xb5\xd8\xad\':True,\'\xd8\xb5\xd8\xad\xd9\x8a\xd8\xad\':True,\'\xd8\xa7\xd9\x8a\xd8\xac\xd8\xa7\xd8\xa8\xd9\x8a\':True,\'\xd8\xa5\xd9\x8a\xd8\xac\xd8\xa7\xd8\xa8\xd9\x8a\':True,\n\t\t\t\t\'0\':False,\'false\':False,\'False\':False,\'FALSE\':False,\'no\':False,\'No\':False,\'NO\':False,\'not\':False,\'Not\':False,\'NOT\':False,\'\xd9\x83\xd9\x84\xd8\xa7\':False,\'\xd8\xae\xd8\xb7\xd8\xa3\':False,\'\xd8\xae\xd8\xb7\xd8\xa7\':False,\'\xd8\xae\xd8\xa7\xd8\xb7\xd8\xa6\':False,\'\xd8\xb3\xd9\x84\xd8\xa8\xd9\x8a\':False}\n\n\tfor field in source[\'fields\']:\n\t\tif field[\'type\'] == \'String\':\n\t\t\tsource[\'data\'][field[\'name\']] = source[\'data\'][field[\'name\']].apply(lambda x: str(x))\n\t\telif field[\'type\'] == \'Number\':\n\t\t\tsource[\'data\'][field[\'name\']] = source[\'data\'][field[\'name\']].apply(pd.to_numeric, errors=\'raise\')\n\t\telif field[\'type\'] == \'Date\':\n\t\t\tsource[\'data\'][field[\'name\']] = source[\'data\'][field[\'name\']].apply(pd.to_datetime(format=field[\'format\'], exact=False), errors=\'raise\')\n\t\telif field[\'type\'] == \'Bool\':\n\t\t\tsource[\'data\'][field[\'name\']] = source[\'data\'][field[\'name\']].map(boolean_map)\n\t\n\treturn source[\'data\']\n\ndef translate_fields(source):\n\tpython_path = sys.executable\n\tfrom_code = "en"\n\tto_code = "ar"\n\tdownload_path = os.path.join(os.path.dirname(python_path), \'static\\\\translate-en_ar-1_0.argosmodel\')\n\targostranslate.package.install_from_path(download_path)\n\n\t# Translate\n\tinstalled_languages = argostranslate.translate.get_installed_languages()\n\tfrom_lang = list(filter(\n\t\tlambda x: x.code == from_code,\n\t\tinstalled_languages))[0]\n\tto_lang = list(filter(\n\t\tlambda x: x.code == to_code,\n\t\tinstalled_languages))[0]\n\ttranslation = from_lang.get_translation(to_lang)\n\n\tfor field in source[\'fields\']:\n\t\tif field[\'format\'] == \'translate\':\n\t\t\tsource[\'data\'][field[\'name\']] = source[\'data\'][field[\'name\']].apply(lambda x: translation.translate(x))\n\n\treturn source[\'data\']\n\nfor col in config[\'collections\']:\n\t#select required columns from dataframe\n\tcol[\'data\'] = df.iloc[:,col[\'fields_indecies\']]\n\t#change columns names\n\tcol[\'data\'] = col[\'data\'].set_axis(col[\'fields_names\'], axis=\'columns\')\n\t#remove duplicate rows based on identity_fields\n\tcol[\'data\'] = col[\'data\'].drop_duplicates(col[\'identity_fields\'],keep= \'first\')\n\t#add _key column\n\tcol[\'data\'][\'_key\'] = col[\'data\'].apply(lambda x: generate_key(), axis=1)\n\t#add _active column\n\tcol[\'data\'] = col[\'data\'].assign(_active=True)\n\t#cast collection fields to type and format\n\tcol[\'data\'] = cast_fields(col)\n\t#translate collection fields if needed\n\tcol[\'data\'] = translate_fields(col)\n\tprint(\'\\n\' + col[\'name\'] + \' data:\\n---------------------------\\n\')\n\tprint(col[\'data\'])\n\nfor edge in config[\'edges\']:\n\t#select required columns from dataframe\n\tedge[\'data\'] = df.iloc[:,edge[\'fields_indecies\']]\n\t#change columns names\n\tedge[\'data\'] = edge[\'data\'].set_axis(edge[\'fields_names\'], axis=\'columns\')\n\tfor col in config[\'collections\']:\n\t\tif col[\'index\'] == edge[\'from_col\']:\n\t\t\t#add collection _key column as _from to edge\n\t\t\tedge[\'data\'] = edge[\'data\'].join(col[\'data\'][\'_key\'])\n\t\t\tedge[\'data\'] = edge[\'data\'].rename({\'_key\':\'_from\'}, axis=\'columns\')\n\t\t\t#replacing nan _from and _to fields with the first available key\n\t\t\tedge[\'data\'][\'_from\'] = edge[\'data\'][\'_from\'].fillna(method=\'ffill\')\n\t\t\t#adding collection name as prefix for the _from field\n\t\t\tedge[\'data\'][\'_from\'] = edge[\'data\'][\'_from\'].apply(lambda x: f"{col[\'name\']}/{x}")\n\t\telif col[\'index\'] == edge[\'to_col\']:\n\t\t\t#add collection _key column as _to to edge\n\t\t\tedge[\'data\'] = edge[\'data\'].join(col[\'data\'][\'_key\'])\n\t\t\tedge[\'data\'] = edge[\'data\'].rename({\'_key\':\'_to\'}, axis=\'columns\')\n\t\t\t#replacing nan _from and _to fields with the first available key\n\t\t\tedge[\'data\'][\'_to\'] = edge[\'data\'][\'_to\'].fillna(method=\'ffill\')\n\t\t\t#adding collection name as prefix for the _to field\n\t\t\tedge[\'data\'][\'_to\'] = edge[\'data\'][\'_to\'].apply(lambda x: f"{col[\'name\']}/{x}")\n\t\n\t#replacing nan _from and _to fields with the first available key\n\tedge[\'data\'][\'_from\'] = edge[\'data\'][\'_from\'].fillna(method=\'ffill\')\n\tedge[\'data\'][\'_to\'] = edge[\'data\'][\'_to\'].fillna(method=\'ffill\')\n\t\n\t#remove duplicate rows based on identity_fields, _from and _to\n\tedge[\'data\'] = edge[\'data\'].drop_duplicates(edge[\'identity_fields\'] + [\'_from\',\'_to\'],keep= \'first\')\n\t\n\t#cast edge fields to type and format\n\tedge[\'data\'] = cast_fields(edge)\n\t#translate edge fields if needed\n\tedge[\'data\'] = translate_fields(edge)\n\n\t#add _key column\n\tedge[\'data\'][\'_key\'] = edge[\'data\'].apply(lambda x: generate_key(), axis=1)\n\tprint(\'\\n\' + edge[\'name\'] + \' data:\\n---------------------------\\n\')\n\tprint(edge[\'data\'])\n\n\nclient = ArangoClient(arango_host)\ndb = client.db(arango_database, arango_username, arango_password)\nfor col in config[\'collections\']:\n\tarango_collection = db.collection(col[\'name\'])\n\tarango_collection.import_bulk(json.loads(col[\'data\'].to_json(orient=\'records\')))\n\nfor edge in config[\'edges\']:\n\tarango_collection = db.collection(edge[\'name\'])\n\tarango_collection.import_bulk(json.loads(edge[\'data\'].to_json(orient=\'records\')))\n\nprint(f\'Done in {str(time.time()-start_time)} seconds.\')'
:comment
SET mypath=%0
SET "pypath=%mypath%.py"
echo %mypath%
C:\Users\Public\python\python.exe C:\Users\Public\python\Lib\parse_import_batch.py %mypath%
C:\Users\Public\python\python.exe %pypath%
pause
